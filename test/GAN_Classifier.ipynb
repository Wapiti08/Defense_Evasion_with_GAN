{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration for data features and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset: tf_idf_part (333, 688)\n",
    "# algorithms need to test: SVC，DecisionTreeClassifier，RidgeClassifierCV, SGDClassifier, MLPClassifier\n",
    "# generator : input_shape：688（features dims）+20（noise dims）   hidden layer nodes：256  output layer nodes：128\n",
    "# subsititude detector: 128 - 256 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules from tensorflow while not keras directly to advoid some problems in training\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Maximum, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifierCV, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-65c9ab00072c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtf_idf_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../dataset/training_data/features_ran_part.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# bypass the label column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf_part\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'types' is not defined"
     ]
    }
   ],
   "source": [
    "# get the data from partial TF-IDF features\n",
    "tf_idf_part = pd.read_pickle('../dataset/training_data/features_ran_part.pkl')\n",
    "# bypass the label column\n",
    "types(tf_idf_part.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the GAN by stacking the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-5c36f14477e5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-5c36f14477e5>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def __init__(self, generator, discriminator, data_path = ,):\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam \n",
    "\n",
    "class RanGAN():\n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        self.data = pd.read_pickle(self.data_path)\n",
    "        self.noise_dims = 100\n",
    "        self.features_dims = 688\n",
    "        self.n_epochs = 50\n",
    "        self.n_batch = 32\n",
    "        # choose the number based on the total number of features\n",
    "        self.hide_layers = 128\n",
    "        self.geneator_layers = [self.features_dims + self.noise_dims, ]\n",
    "        \n",
    "        \n",
    "        optimizer = Adam(0.0001, 0.5)\n",
    "        \n",
    "        # build and compile the detector\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer , metrics=['accuracy'])\n",
    "        \n",
    "        # build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        # generate the adversarial ransomware examples\n",
    "        example = Input(shape = (self.features_dims))\n",
    "        noise = Input(shape = (self.noise_dims))\n",
    "        input_shape = [example, noise]\n",
    "        adv_ran = self.generator(input_shape)\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(adv_ran)\n",
    "        \n",
    "        # the combined model (stacked generator and discriminator)\n",
    "        self.combined = Model(input_shape, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer = optimizer)\n",
    "        \n",
    "        # for the full GAN, only train the generator\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        # Building a simple Generator network\n",
    "        def build_generator(self):\n",
    "            example = Input(shape=(self.features_dims,))\n",
    "            noise = Input(shape = (self.noise_dims))\n",
    "            # create the concatenated dims for input example\n",
    "            adv_example = Concatenate(axis=1)([example, noise])\n",
    "            for dim in [256, 128]:\n",
    "                x = Dense(dim)(adv_example)\n",
    "            x = Activation(activation='sigmoid')(x)\n",
    "            x = Maximum()([example, x])\n",
    "            # multiple input api\n",
    "            generator = Model(input =[example, noise], output = x, name = 'Generator')\n",
    "            generator.summary()\n",
    "            return generator\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Building a simple Discriminator network\n",
    "        def build_discriminator(self):\n",
    "            input_example = Input(shape=(feature_dims,))\n",
    "            for dim in [feature_dims, 1]:\n",
    "                x = Dense(dim)(input_example)\n",
    "            validity = Activation(activation='sigmoid')(x)\n",
    "            \n",
    "            discriminator = Model(input_example, validity, name='Discriminator')\n",
    "            discriminator.summary()\n",
    "            \n",
    "            return discriminator\n",
    "\n",
    "        \n",
    "        \n",
    "        def load_data(self):\n",
    "            # including features values and labels\n",
    "            x_ran, y_ran, x_ben, y_ben = self.data[self.data['label']==1].iloc[:,:-1], \n",
    "                                         self.data[self.data['label']==1].iloc[:,-1], \n",
    "                                         self.data[self.data['label']==0].iloc[:,:-1], \n",
    "                                         self.data[self.data['label']==0].iloc[:,-1]\n",
    "\n",
    "            return x_ran, y_ran, x_ben, y_ben\n",
    "            \n",
    "        \n",
    "        def train_gan(self, generator, discriminator):\n",
    "            \n",
    "            # calculate the number of batches per epoch\n",
    "            batches_per_epoch = int(self.dataset.shape[0]/self.n_batch)\n",
    "            # calculate the number of training iterations\n",
    "            n_steps = batches_per_epoch * self.n_epochs\n",
    "            \n",
    "            # Load and Split the dataset\n",
    "            (x_ran, y_ran), (x_ben, y_ben) = self.load_data()\n",
    "        \n",
    "            \n",
    "            for step in range(n_steps):\n",
    "                \n",
    "                # -----------------------\n",
    "                # Train Discriminator\n",
    "                # -----------------------\n",
    "                \n",
    "                # 1.select the random ransomware examples with batch size\n",
    "                idx = np.random.randint(0, x_train_ran.shape[0], n_batch)\n",
    "                x_ran_batch = x_train_ran[idx]\n",
    "                \n",
    "                # sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (n_batch, self.noise_dims))\n",
    "                \n",
    "                idx = np.random.randint(0, x_ran_batch.shape)\n",
    "                \n",
    "                # 2. generate the batch of new ransomware examples\n",
    "                gen_examples = self.generator.predict([x_ran_batch, noise])\n",
    "                \n",
    "                y_ran_batch =  self.discriminator.predict(np.ones(gen_examples.shape)*(gen_examples > 0.5))\n",
    "                \n",
    "                # 3. Train the discriminator using both fake and real examples\n",
    "                # it will update the D's wrights by labeling all real examples as 1\n",
    "                # and the fake images as 0\n",
    "                d_loss_real = discriminator.train_on_batch(gen_examples, y_ran_batch)\n",
    "                d_loss_fake = discriminator.train_on_batch()\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                # ------------------------\n",
    "                # Train Generator\n",
    "                # ------------------------\n",
    "                \n",
    "                # 4. Generate another batch of fake examples\n",
    "                \n",
    "                \n",
    "                # 5. Train the full GAN model using fake examples only\n",
    "                # update only the G's weights by labeling all fake examples as 1\n",
    "                fake = generator.predict()\n",
    "                result = discriminator.predict(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    acgan = RanGAN()\n",
    "    acgan.train(epoch=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitdbd76ed984a5488496eb976b9e8b3b8e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
